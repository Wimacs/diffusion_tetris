{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.gen.edm import EDM\n",
    "from models.gen.blocks import UNet\n",
    "import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from data.data import SequencesDataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3\n",
    "context_length = 4\n",
    "actions_count = 5\n",
    "batch_size = 1\n",
    "num_workers = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "FPS = 1\n",
    "\n",
    "# For Mac OS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "ROOT_PATH = \"../\"\n",
    "def local_path(path):\n",
    "    return os.path.join(ROOT_PATH, path)\n",
    "MODEL_PATH = local_path(\"models/model_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edm = EDM(\n",
    "    p_mean=-1.2,\n",
    "    p_std=1.2,\n",
    "    sigma_data=0.5,\n",
    "    model=UNet((input_channels) * (context_length + 1), 3, None, actions_count, context_length),\n",
    "    context_length=context_length,\n",
    "    device=device\n",
    ")\n",
    "edm.load_state_dict(torch.load(MODEL_PATH, map_location=device)[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../training_data/snapshots'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m transform_to_tensor = transforms.Compose([\n\u001b[32m      2\u001b[39m     transforms.ToTensor(),\n\u001b[32m      3\u001b[39m     transforms.Normalize((\u001b[32m.5\u001b[39m,\u001b[32m.5\u001b[39m,\u001b[32m.5\u001b[39m), (\u001b[32m.5\u001b[39m,\u001b[32m.5\u001b[39m,\u001b[32m.5\u001b[39m))\n\u001b[32m      4\u001b[39m ])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mSequencesDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_data/snapshots\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_data/actions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform_to_tensor\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/diffusion_tetris/src/data/data.py:20\u001b[39m, in \u001b[36mSequencesDataset.__init__\u001b[39m\u001b[34m(self, images_dir, actions_path, seq_length, transform)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     13\u001b[39m     images_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     transform: Optional[Any] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     19\u001b[39m     paths = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m         [ item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m item.endswith(\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m     21\u001b[39m         key=\u001b[38;5;28;01mlambda\u001b[39;00m item: \u001b[38;5;28mint\u001b[39m(item.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m])\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.images_dir = images_dir\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(actions_path) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../training_data/snapshots'"
     ]
    }
   ],
   "source": [
    "transform_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "])\n",
    "\n",
    "dataset = SequencesDataset(\n",
    "    images_dir=local_path(\"tetris/data/snapshots\"),\n",
    "    actions_path=local_path(\"tetris/data/actions\"),\n",
    "    seq_length=context_length,\n",
    "    transform=transform_to_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b80830726480bbe07a17e4dab171c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aaeca25bb64b668eb2eb08a419da22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output, Image as iImage\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        self.action = 0\n",
    "        self.is_running = False\n",
    "        self.frame_number = 0\n",
    "        self.gen_imgs = None\n",
    "        self.actions = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.frame_number = 0\n",
    "        self.is_running = False\n",
    "        self.gen_imgs = None\n",
    "        self.actions = None\n",
    "\n",
    "state = State()\n",
    "\n",
    "def on_button_click(input_action):\n",
    "    state.action = input_action\n",
    "\n",
    "# Create buttons\n",
    "left_button = widgets.Button(description='Left')\n",
    "right_button = widgets.Button(description='Right')\n",
    "up_button = widgets.Button(description='Up')\n",
    "down_button = widgets.Button(description='Down')\n",
    "start_button = widgets.Button(description='Start')\n",
    "stop_button = widgets.Button(description='Stop')\n",
    "\n",
    "directions = {\n",
    "    0: \"Right\",\n",
    "    1: \"Left\",\n",
    "    2: \"Up\",\n",
    "    3: \"Down\"\n",
    "}\n",
    "\n",
    "# Set up button callbacks\n",
    "right_button.on_click(lambda b: on_button_click(0))\n",
    "left_button.on_click(lambda b: on_button_click(1))\n",
    "up_button.on_click(lambda b: on_button_click(2))\n",
    "down_button.on_click(lambda b: on_button_click(3))\n",
    "\n",
    "# Display buttons horizontally\n",
    "buttons = widgets.HBox([left_button, widgets.VBox([up_button, down_button]), right_button, start_button, stop_button])\n",
    "\n",
    "button_output = widgets.Output()\n",
    "image_output = widgets.Output()\n",
    "\n",
    "with button_output:\n",
    "    display(buttons)\n",
    "\n",
    "def get_np_img(tensor: torch.Tensor) -> np.ndarray:\n",
    "    return (tensor * 127.5 + 127.5).long().clip(0,255).permute(1,2,0).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "def render_frame():\n",
    "    if not state.is_running:\n",
    "        return\n",
    "        \n",
    "    if state.frame_number >= 80:\n",
    "        stop_rendering()\n",
    "        return\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize on first frame\n",
    "    if state.frame_number == 0:\n",
    "        index = random.randint(0, len(dataset) - 1)\n",
    "        img, last_imgs, actions = dataset[index]\n",
    "        state.gen_imgs = last_imgs.clone().to(device)\n",
    "        state.actions = actions.to(device)\n",
    "    \n",
    "    # Generate new frame\n",
    "    state.actions = torch.concat((state.actions, torch.tensor([state.action], device=device)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_img = edm.sample(\n",
    "            10,\n",
    "            state.gen_imgs[0].shape,\n",
    "            state.gen_imgs[-context_length:].unsqueeze(0),\n",
    "            state.actions[-context_length:].unsqueeze(0)\n",
    "        )[0]\n",
    "    \n",
    "    state.gen_imgs = torch.concat([state.gen_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "    \n",
    "    # Display frame\n",
    "    display_img = get_np_img(gen_img)\n",
    "    buffer = io.BytesIO()\n",
    "    Image.fromarray(display_img).resize((360, 360), Image.Resampling.LANCZOS).save(buffer, format='PNG')\n",
    "    \n",
    "    with image_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Direction: {directions[state.action]}')\n",
    "        print(f'Frame: {state.frame_number + 1}/80')\n",
    "        display(iImage(data=buffer.getvalue()))\n",
    "    \n",
    "    state.frame_number += 1\n",
    "    \n",
    "    # Maintain frame rate\n",
    "    elapsed_time = time.time() - start_time\n",
    "    delay = max(0, frame_time - elapsed_time)\n",
    "    \n",
    "    # Schedule next frame\n",
    "    if state.is_running:\n",
    "        timer = time.time() + delay\n",
    "        while time.time() < timer:\n",
    "            pass\n",
    "        render_frame()\n",
    "\n",
    "def start_rendering(b):\n",
    "    if state.is_running:\n",
    "        return\n",
    "    state.reset()\n",
    "    state.is_running = True\n",
    "    render_frame()\n",
    "\n",
    "def stop_rendering(b=None):\n",
    "    state.reset()\n",
    "    with image_output:\n",
    "        clear_output(wait=True)\n",
    "        print('Stopped rendering')\n",
    "\n",
    "start_button.on_click(start_rendering)\n",
    "stop_button.on_click(stop_rendering)\n",
    "\n",
    "# Initialize constants\n",
    "frame_time = 1 / FPS\n",
    "\n",
    "display(button_output)\n",
    "display(image_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
